#+TITLE: Tiny Search Engine (TSE)

This project is a small-scale search engine implemented in C for Dartmouth College's CS50 class, Software Design and Implementation. It consists of three main components: a crawler, an indexer, and a querier. Each component is organized within its respective directory inside the `tse` directory. Additionally, you'll find a `utils` directory containing all the necessary C modules and interfaces, as well as a `lib` directory for the compiled library.

* Project Structure

  - **crawler**: Contains the source code for the web crawler component. The crawler retrieves web pages from a given seed URL and saves them to a local directory for further processing.
  
  - **indexer**: Houses the implementation for the indexer component. This module processes the crawled web pages, extracts relevant information, and creates an index for efficient querying.

  - **querier**: Implements the querying functionality of the search engine. Given a query, the querier component searches through the indexed data to retrieve relevant documents.

  - **utils**: This directory contains various utility functions and data structures used across the project. It includes both C source files and header files defining interfaces for shared functionality.

  - **lib**: Contains the compiled library generated during the build process. This library includes the compiled versions of the utility modules used by the crawler, indexer, and querier.

* Getting Started

  To get started with the Tiny Search Engine project, follow these steps:

  1. Clone the repository to your local machine.
  2. Navigate to the `tse` directory.
  3. Compile the project using the provided makefiles.
  4. Once compiled, you can run each component individually by navigating to its respective directory and executing the compiled binary.

* Usage

  Each component of the Tiny Search Engine project can be run with specific command-line arguments. Below is a brief overview of each component's usage:

  - **Crawler**

    ```bash
    ./crawler [seed URL] [output directory] [depth]
    ```

    - `seed URL`: The starting URL from which the crawler will begin crawling.
    - `output directory`: The directory where the crawled web pages will be saved.
    - `depth`: The maximum depth to which the crawler will crawl from the seed URL.

  - **Indexer**

    ```bash
    ./indexer [crawler output directory] [index file]
    ```

    - `crawler output directory`: The directory containing the crawled web pages.
    - `index file`: The filename where the indexer will save the generated index.

  - **Querier**

    ```bash
    ./querier [index file] [page directory]
    ```

    - `index file`: The filename of the index file generated by the indexer.
    - `page directory`: The directory containing the crawled web pages.

* Dependencies

  This project relies on the standard C library and does not have any external dependencies beyond that.

