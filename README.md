# Tiny Search Engine (TSE)

This project is a small-scale search engine implemented in C for Dartmouth College's CS50 class, Software Design and Implementation. It consists of three main components: a crawler, an indexer, and a querier. Each component is organized within its respective directory (additionally, there is a parallel_indexer directory which contains a parellized implementation of the indexer). Additionally, you'll find a `utils` directory containing all the necessary C modules and interfaces, as well as a `lib` directory for the compiled library.

* Project Structure

  - **crawler**: Contains the source code for the web crawler component. The crawler retrieves web pages from a given seed URL and saves them to a local directory for further processing.
  
  - **indexer**: Houses the implementation for the indexer component. This module processes the crawled web pages, extracts relevant information, and creates an index for efficient querying.

  - **querier**: Implements the querying functionality of the search engine. Given a query, the querier component searches through the indexed data to retrieve relevant documents.

  - **utils**: This directory contains various utility functions and data structures used across the project. It includes both C source files and header files defining interfaces for shared functionality.

  - **lib**: Contains the compiled library generated during the build process. This library includes the compiled versions of the utility modules used by the crawler, indexer, and querier.
  
  - `makeall.sh`: This script may be run to make all of the components of the project.
  
  - `cleanall.sh`: This script may be run to clean all of the components of the project.
  
  - `run.sh`: This script may be run to run the tiny search engine (it also makes the project and cleans up at termination).
  

* Usage

  Each component of the Tiny Search Engine project can be run with specific command-line arguments. Below is a brief overview of each component's usage:

  - **Crawler**

    ```bash
    ./crawler [seed URL] [output directory] [depth]
    ```

    - `seed URL`: The starting URL from which the crawler will begin crawling.
    - `output directory`: The directory where the crawled web pages will be saved.
    - `depth`: The maximum depth to which the crawler will crawl from the seed URL.

  - **Indexer**

    ```bash
    ./indexer [crawler output directory] [index file]
    ```

    - `crawler output directory`: The directory containing the crawled web pages.
    - `index file`: The filename where the indexer will save the generated index.
	
  - **Parallel**

    ```bash
    ./indexer [crawler output directory] [index file] [number of threads]
    ```

    - `crawler output directory`: The directory containing the crawled web pages.
    - `index file`: The filename where the indexer will save the generated index.
	- `number of threads`: The number of parallel threads to be created.

  - **Querier**

    ```bash
    ./querier [page directory] [index file] [[-q]]
    ```
	- `page directory`: The directory containing the crawled web pages.
    - `index file`: The filename of the index file generated by the indexer.
	- `-q`: Allows queries to be loaded quietly from a file.
